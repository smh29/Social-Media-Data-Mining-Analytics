source('src/R/default.R')

# Read in the stemmed post terms and metadata created by
# process_stackexchange_xml.py .
posts = read.table(gzfile('data/stack_exchange/posts.tsv.gz'),
  sep='\t', stringsAsFactors=F,
  col.names=c('id', 'post.type.id', 'parent.id',
    'owner.user.id', 'creation.date', 'view.count',
    'favorite.count', 'tags', 'terms'))

# Consider only questions, no answers for simplicity.
posts = subset(posts, post.type.id == 1)
posts$favorite.count[is.na(posts$favorite.count)] = 0

word.frequencies = read.table(gzfile('data/stack_exchange/vocabulary.tsv.gz'),
  sep='\t', col.names=c('term', 'frequency'))


##############################################################################
# Plot the term Zipf (rank) distribution                                     #
##############################################################################

# This is only for the questions
words = unlist(strsplit(posts$terms, ' '))
word.freqs = table(words)
word.freqs = word.freqs[order(word.freqs, decreasing=TRUE)]
ggplot(data.frame(frequency=word.freqs),
    aes(x=1 : length(word.freqs), y=frequency)) +
  geom_line(size=defaults$line.size) +
  scale_x_log10('Term rank',
    breaks=trans_breaks('log10', function(x) 10 ^ x),
    labels=trans_format('log10', math_format(10 ^ .x))) +
  scale_y_log10('Frequency',
    breaks=trans_breaks('log10', function(x) 10 ^ x),
    labels=trans_format('log10', math_format(10 ^ .x)))

# Alternative way, using the data generated by process_stackexchange_xml.py
word.frequencies = word.frequencies[order(word.frequencies$frequency,
    decreasing=TRUE),]
ggplot(word.frequencies, aes(x=1 : nrow(word.frequencies), y=frequency)) +
  geom_line(size=defaults$line.size) +
  scale_x_log10('Term rank',
    breaks=trans_breaks('log10', function(x) 10 ^ x),
    labels=trans_format('log10', math_format(10 ^ .x))) +
  scale_y_log10('Frequency',
    breaks=trans_breaks('log10', function(x) 10 ^ x),
    labels=trans_format('log10', math_format(10 ^ .x)))

# This is not going to be a power-law distribution beyond ranks of about
# 100-1000, but this has been even observed: http://www.britannica.com/EBchecked/topic/709359/Zipfs-law
# But this is also possible due to the fact that our terms are not really
# words.


##############################################################################
# Figure 4.2                                                                 #
# Plot the term frequency distribution                                       #
##############################################################################

# Do it on word.freqs that were calculated before restricting to questions only
distrib = ddply(data.frame(frequency=word.freqs), .(frequency),
  summarize, count=nrow(piece))
fit = lm(log(count) ~ log(frequency), subset(distrib, frequency < 1e2))
print(fit)
# Coefficients:
#        (Intercept)  log(frequency)
#              9.818          -1.448
ggplot(distrib, aes(x=frequency, y=count)) +
  geom_line(size=defaults$line.size) +
  geom_line(data={
      x = 10 ^ c(0, 2)
      data.frame(x=x, y=10 ^ 3.5 * (x / min(x)) ^ fit$coefficients[2])
    }, aes(x=x, y=y), size=defaults$line.size, alpha=0.5) +
  scale_x_log10('Number of times a term occurs',
    breaks=trans_breaks('log10', function(x) 10 ^ x),
    labels=trans_format('log10', math_format(10 ^ .x))) +
  scale_y_log10('Number of terms',
    breaks=trans_breaks('log10', function(x) 10 ^ x),
    labels=trans_format('log10', math_format(10 ^ .x)))


##############################################################################
# Plot the tag Zipf (rank) distribution                                      #
##############################################################################

tags = unlist(strsplit(posts$tags, ' '))
tag.frequencies = table(tags)
tag.frequencies = tag.frequencies[order(tag.frequencies, decreasing=TRUE)]
ggplot(data.frame(frequency=tag.frequencies),
    aes(x=1 : length(tag.frequencies), y=frequency)) +
  geom_line(size=defaults$line.size) +
  geom_line(data={
      x = 10 ^ c(0.5, 2)
      data.frame(x=x, y=10 ^ 2.5 * (x / min(x)) ^ -1.0)
    }, aes(x=x, y=y), size=defaults$line.size, alpha=0.5) +
  scale_x_log10('Tag rank',
    breaks=trans_breaks('log10', function(x) 10 ^ x),
    labels=trans_format('log10', math_format(10 ^ .x))) +
  scale_y_log10('Frequency',
    breaks=trans_breaks('log10', function(x) 10 ^ x),
    labels=trans_format('log10', math_format(10 ^ .x)))


##############################################################################
# Figure 4.7                                                                 #
# Plot the tag frequency distribution                                        #
##############################################################################

distrib = ddply(data.frame(frequency=tag.frequencies), .(frequency), summarize,
  count=nrow(piece))
fit = lm(log(count) ~ log(frequency), subset(distrib, frequency < 10 ^ 1.5))
print(fit)
# Coefficients:
#         (Intercept)  log(frequency)
#               6.368          -1.405
ggplot(distrib, aes(x=frequency, y=count)) +
  geom_line(size=defaults$line.size) +
  geom_line(data={
      x = 10 ^ c(0, 1.5)
      data.frame(x=x, y=10 ^ 2.2 * (x / min(x)) ^ fit$coefficients[2])
    }, aes(x=x, y=y), size=defaults$line.size, alpha=0.5) +
  scale_x_log10('Number of times a tag occurs',
    breaks=trans_breaks('log10', function(x) 10 ^ x),
    labels=trans_format('log10', math_format(10 ^ .x))) +
  scale_y_log10('Number of tags',
    breaks=trans_breaks('log10', function(x) 10 ^ x),
    labels=trans_format('log10', math_format(10 ^ .x)))


##############################################################################
# Listing 4.2                                                                #
# Cluster the posts based on cosine similarities of their bags of words      #
##############################################################################

library(tm)
library(Matrix)
library(ggdendro)
library(wordcloud)

# Take all the posts
chosen.posts = 1 : nrow(posts)

# Take only a random sample from the posts
chosen.posts = sample(1 : nrow(posts), 500)

corpus = Corpus(VectorSource(posts$terms[chosen.posts]))
term.doc.matrix = TermDocumentMatrix(corpus,
  control=list(weighting=weightTfIdf))

# We treat the term-document matrix as a sparse matrix, and
# calculate the post similarities by matrix multiplication.
tdsm = sparseMatrix(i=term.doc.matrix$i, j=term.doc.matrix$j,
  x=term.doc.matrix$v, dims=c(term.doc.matrix$nrow, term.doc.matrix$ncol))

# Normalize the column vectors of tdsm by their lengths
normalized.term.vectors = tdsm %*% Diagonal(x=1 / sqrt(colSums(tdsm ^ 2)))

# Calculate the similarities of posts using the term vectors, and create
# dissimilarities by flipping their values
# To keep the matrix sparse we only take the negative of the similarities to
# flip their order
post.dissimilarities = -crossprod(normalized.term.vectors)

##############################################################################
# Listing 4.3                                                                #
##############################################################################

# Perform a hierarchical agglomerative clustering with average linkage.
h = hclust(as.dist(post.dissimilarities), method='average')

# Convert the merging heights back to a dissimilarity scale of [0; 1].
h$height = 1 + h$height

# Plot the dendrogram of post similarities for the full dataset
plot(h, hang=0.02, labels=FALSE, main='', sub='', xlab='', ylab='')

h.dendr = as.dendrogram(h)
plot(cut(h.dendr, h=0.9)$upper, leaflab='none', ylim=c(0.9, 1))


##############################################################################
# Listing 4.4                                                                #
# Plot the upper part of the dendrogram                                      #
##############################################################################

# Convert the result of clustering into a dendrogram that cut can handle.
h.dendr = as.dendrogram(h)

# Cut the dendrogram, and keep only the merges that were done
# at a distance 0.9 or higher.
h.dendr.upper = cut(h.dendr, h=0.9)$upper

# Plot only this upper part.
plot(h.dendr.upper, leaflab='none', ylim=c(0.9, 1))

# Select the branches with a left click, exit with a right click
identified.branches = identify(h, N=10, MAXCLUSTER=length(h$height) + 1)

# Create tag clouds for the branches that we selected.
library(wordcloud)
invisible(lapply(seq_along(identified.branches), function(i) {
      leaves = identified.branches[[i]]
      # The number of times a term appears in the leaf posts
      words = rowSums(as.matrix(term.doc.matrix[, leaves]))
      words = words[words > 0]
      words = words / max(words)		# normalize the counts
      wordcloud(names(words), words, max.words=50,
        scale=c(4, 0.5))		# plot the word cloud
      # Create a word cloud also for the tags associated with
      # the posts
      tags = unlist(strsplit(posts$tags[chosen.posts[leaves]], ' '))
      tag.counts = table(tags)
      tag.counts = tag.counts / max(tag.counts)
      wordcloud(names(tag.counts), tag.counts, max.words=10,
        scale=c(4, 0.5))
    }))


##############################################################################
# Create text and tag clouds for some selected branches                      #
##############################################################################

identified.branches = identify(h, N=10, MAXCLUSTER=length(h$height) + 1)
# Print the links to the selected posts so that we can open them in a browser
print.stackexchange.links = function(h, identified.branches) {
  links = lapply(identified.branches, function(l)
      sprintf('http://scifi.stackexchange.com/questions/%d',
        posts[chosen.posts[l], c('id')]))
  cat(links[[1]], sep='\n')
}
print.stackexchange.links(h, identified.branches[3])


##############################################################################
# Listing 4.5                                                                #
# The cluster size distribution under a given height                         #
##############################################################################

# Cut the full dendrogram at different heights, and determine the size
# distribution of the topic clusters under the cuts
h.dendr = as.dendrogram(h)
cut.heights = c(0.5, 0.7, 0.9)
branch.size.distrib = data.frame()
for (cut.height in cut.heights) {
  # The cut doesn't work for large dendrograms, try to rerun it using the
  # h$merge structure directly by passing it to a Python script for speed
  # Cut the final dendrogram to see what clusters we had at that point during
  # the run of the hierarchical clustering
  dendr.cut = cut(h.dendr, h=cut.height)
  branch.sizes = sapply(dendr.cut$lower, function(b) attr(b, 'members'))
  # Calculate the size distribution on this branch
  size.distrib = ddply(data.frame(size=branch.sizes), .(size),
    summarize, count=nrow(piece))
  # Store the distribution together with a column storing the height of cut
  branch.size.distrib = rbind(branch.size.distrib,
    data.frame(size.distrib, cut.height=cut.height))
}
branch.size.distrib$cut.height = as.factor(branch.size.distrib$cut.height)
ggplot(branch.size.distrib) +
  geom_line(aes(x=size, y=count, group=cut.height),
    size=defaults$line.size / 2, alpha=0.8) +
  geom_point(data=create.log.line.markers(branch.size.distrib, 5,
      by=c('cut.height')),
    aes(x=x, y=y, shape=cut.height),
    size=defaults$point.size, fill='white') +
  scale_x_log10('Number of posts on a branch',
    breaks=trans_breaks('log10', function(x) 10 ^ x),
    labels=trans_format('log10', math_format(10 ^ .x))) +
  scale_y_log10('Number of branches',
    breaks=trans_breaks('log10', function(x) 10 ^ x),
    labels=trans_format('log10', math_format(10 ^ .x))) +
  scale_shape_manual(name='Cut height',
    values=21 : 23)


##############################################################################
# Listing 4.6                                                                #
# Look at how users post into different categories                           #
##############################################################################

# Create a list of tag vectors for all post tags
all.tags = strsplit(posts$tags, ' ')
# Create a data frame where the user.id column is the poster's user ID,
# and the tag column is all the tags they used on any of their posts
users.tags = do.call(rbind, lapply(seq_along(all.tags), function(i)
      data.frame(
        user.id=posts$owner.user.id[i],
        tag=all.tags[[i]])))
users.tags$tag = as.character(users.tags$tag)       # convert factors to strings
# For each user, count their tag frequencies, normalize them, and sort in
# in decreasing order of normalized frequency so that they correspond to ranks
users.ranked.tags = ddply(users.tags, .(user.id), function(df) {
    tag.freqs = as.vector(table(df$tag))    # count the tags
    tag.freqs = tag.freqs / nrow(df)		    # normalize the frequencies
    tag.freqs = tag.freqs[order(tag.freqs, decreasing=TRUE)]  # rank them
    return(data.frame(rank=1 : length(tag.freqs), freqs=tag.freqs))
  })

# Average the relative frequencies as a function of the ranks of the tags
mean.ranked.tags = ddply(users.ranked.tags, .(rank), summarize,
  mean.freq=sum(freqs))
mean.ranked.tags = within(mean.ranked.tags, {
    mean.freq = mean.freq / sum(mean.freq)
  })
ggplot(mean.ranked.tags, aes(x=rank, y=mean.freq)) +
  geom_line(size=defaults$line.size) +
  scale_x_log10('The rank of a user\'s tag',
    breaks=trans_breaks('log10', function(x) 10 ^ x),
    labels=trans_format('log10', math_format(10 ^ .x))) +
  scale_y_log10('Fraction of the user\'s posts',
    breaks=trans_breaks('log10', function(x) 10 ^ x),
    labels=trans_format('log10', math_format(10 ^ .x)))

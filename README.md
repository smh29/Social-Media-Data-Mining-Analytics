# Social-Media-Data-Mining-Analytics

![1118824857](https://user-images.githubusercontent.com/4604882/111078543-377ec780-84b3-11eb-925f-1a4e99b37507.jpg)


This is a simple reposting of the code from the book Social Media Data Mining & Analytics by Gabor Szabo, Gungor Polatkan, Oscar Boykin, & Antonios Chalkiopoulos. It includes code in Python, R, & Scala. This code at the time of writing is available only as a zip file from the Wiley site related to this book. But this being seemingly only available there, it is possible the code could be lost from the site (which has happened to the code for other books on the Wiley site & is not a trivial concern). So I am making the code available, as is at the time of download, and adding it to github. The second reason for adding the code is that making it available as is with the directories setup as it looks after unzipping the code from Wiley, adds this set of code to the github system, with the benefits that adds to a directory of code.

The book is a unique take on data analytics with a theme of the unity of human behavior across media platforms and the scientific and computational findings that show the uniformity of power laws across social media platforms of various types. From Wikipedia edits, to Twitter, to the sci-fi/fantasy sub forum on Stack Exchange, to film ratings on movielens, to Amazon Fine Food reviews, to Live Journal to, lastly, a set of scientific documents from the Cora search engine. It uses a 'what, when, where, who, why' format. With 'the who' of social media being the users on wikipedia and twitter as well as posters on stack exchange and other sites, 'the when' focuses on the temporal elements of social media sites, so stufff like the frequency of posts, etc, using ARIMA models as one of multiple approaches to studying time-series, 'the where' and 'the what' being the actual networks that form on social media sites. With the social network section focusing on network science methodology and uses python and R to recreate and view certain network structures from social media sites.'The why' being the overall focus of the book and this code. Though the book is arguably aimed at a business analyst/business data scientist the writing style is much more scientific and methodologically rigorous in its approach than most other similar works. It rises above the rest by focusing on the unity of the nature of human behavior through power laws, and related statistical results, as well as the underlying network structures across all of the websites and datasets studied. Alongside that it has a very useful mathematical formalism that makes the mathematics of social network systems understandable & is still more rigorous than most works of this type.

The code is available directly from the Wiley site here: https://www.wiley.com/en-ca/Social+Media+Data+Mining+and+Analytics-p-9781118824856 but doesn't exist on github in this form. 

The code is a couple of years old now and given the download from wiley doesn't seem to be modified, the code will potentially have errors that are related to the version of python you are using. At a later point I may add slight modifications to the code so that it is updated, but as of now  the code is as is. I have simply collected it here for git access to the code and in case anything happens to the wiley site making the download unavailable (something that has frustratingly happened to me for a different Wiley title, so the concern is not undue). Also, there is a git for this book, but mainly for the part using scalding, not all the rest which is included here. The link for that git is  https://github.com/scalding-io/social-media-analytics but again this is not the same as the code which one gets from Wiley and is being preserved here.

# Instructions for Setup:

Ubuntu 18.04 or above is required, or the use of virtual linux machines if one is using Windows or Mac. So first of all run: _setup/setup.sh_ this will install all the binaries, dependencies and files for python and R and Scala that you will need. This will take a bit of time. Next it is recommended to activate the virtual environment _. venv/bin/activate_ Next to get all the data from wikipedia to stack exchange all in one download run _data/download_all.sh_ this is a huge download. The vast majority of its size comes from the wikipedia edit history, which is around 70GB. For this part to work properly you may need to find the most current _enwiki-stub-meta-history.xml.gz_ file yourself on the wikimedia site. Use the file _get_wikipedia_data.sh_ from the _src/chapter1_ directory. Open it and where it is coded _'curl https://dumps.wikimedia.org/enwiki/$backup_date/enwiki-$backup_date-stub-meta-history.xml.gz \
    -o data/wikipedia/enwiki-stub-meta-history.xml.gz'_ change the dump URL to the most recent one you've found on wikimedia. Make sure it is a _xml.gz_   filetype. Once this section has been edited then _the_download_all.sh_  script in the _setup_ directory will run.This should now give you everything you need to run the code from the book in the various chapter sections. As a final recommendation, once running python if errors come up immediately after running the code from the command line one tip is to run it with python2 rather than python3. 
    

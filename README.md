# Social Media Data Mining & Analytics

![1118824857](https://user-images.githubusercontent.com/4604882/111078543-377ec780-84b3-11eb-925f-1a4e99b37507.jpg)

## Description & Comments:

This is a simple reposting of the code from the book Social Media Data Mining & Analytics by Gabor Szabo, Gungor Polatkan, Oscar Boykin, & Antonios Chalkiopoulos. It includes code in Python, R, & Scala. This code at the time of writing is available only as a zip file from the Wiley site related to this book. But this being seemingly only available there, it is possible the code could be lost from the site (which has happened to the code for other books on the Wiley site & is not a trivial concern). So I am making the code available, as is at the time of download, and adding it to github. The second reason for adding the code is that making it available as is with the directories setup as it looks after unzipping the code from Wiley, adds this set of code to the github system, with the benefits that adds to a directory of code.

The book is a unique take on data analytics with a theme of the unity of human behavior across media platforms and the scientific and computational findings that show the uniformity of power laws across social media platforms of various types. From Wikipedia edits, to Twitter, to the sci-fi/fantasy sub forum on Stack Exchange, to film ratings on movielens, to Amazon Fine Food reviews, to Live Journal to, lastly, a set of scientific documents from the Cora search engine. It uses a 'what, when, where, who, why' format (or as they describe it, a four key characteristics of online services: the users, the social networks, the user actions over time, and the content posted itself). I would describe it as follows, with 'the who' of social media being the users on wikipedia and twitter. 'The when' focuses on the temporal elements of social media site use, so stufff like the frequency of posts, etc. This is analyzed using traditional time-series analytic techniques, for example by using ARIMA models as one technique to study and make predictions, from time-series data. 'The where' and 'the what' being the actual networks that form on social media sites. With the social network section focusing on network science statistics and the methodology which has formed to study social networks. Another aspect of 'the What' & 'the where' are  the sections which focus on text analysis and recommender systems, these focus on actual distinct posts from distinct websites, for example Latent Dirichlet Allocation (LDA) models are used for analyzing topics at a high level using text from various sources already mentioned. Both unsupervised and supervised learning models are used. Movie reviews from movielens are the focus of the recommender systems.'The why' being the overall focus of the book and this code, the book itself and accompanying code.

Though the book is arguably aimed at a business analyst audience, this does not end up being the actual focus and aim of the book, the book would appeal more to those who are genuinely interested in human behavior and its unity and dirrences across various online services. So its main focus is the underlying structure of human behaviour mediated by social media computer systems. This approach to human behaviour actually acts as a uniting factor throughout the book with a heavy focus on the repeating discovery of power laws when analyzing different aspects of social media use and different platforms. It also focuses on the underlying medium of human connection, the transfer of data through man made social networks utilyzing the media of various websites to create connections with each other which again are common across many systems and appear to show some unified aspect of human behaviour generally. The writing style is much more scientific than one would expeect from a book of its kind, and it is  methodologically rigorous in its approach. Alongside that it has a very useful mathematical formalism that makes the mathematics of social network systems and the statistics surrounding power laws and related systems, as well as the elements of fairly advanced probability that surrounds text analysis and topic modelling. For those interested, PDFs of this book are available through a simple google search!Using the text you can follow through these code examples (or by using the zip from Wiley - if it still exists when you are reading this). Though the book came out early 2019 some of it is becoming dated, however I still highly recommend it as a great book on social media analytics and data science. Considering many of the books that have come out on the general subjects of data science/machine learning/social media anlysis/big data analytics since 2015 or so, this is one of the best I have come across, if not the best.

The code is available directly from the Wiley site here: https://www.wiley.com/en-ca/Social+Media+Data+Mining+and+Analytics-p-9781118824856 but doesn't exist on github in this form. 

The code is a couple of years old now and given the download from wiley doesn't seem to be modified, the code will potentially have errors that are related to the version of python you are using. At a later point I may add slight modifications to the code so that it is updated, but as of now  the code is as is. I have simply collected it here for git access to the code and in case anything happens to the wiley site making the download unavailable (something that has frustratingly happened to me for a different Wiley title, so the concern is not undue). Also, there is a git for this book, but mainly for the part using scalding, not all the rest which is included here. The link for that git is  https://github.com/scalding-io/social-media-analytics but again this is not the same as the code which one gets from Wiley and is being preserved here.

## Instructions for Setup:

Ubuntu 18.04 or above is required, or the use of virtual linux machines if one is using Windows or Mac. So first of all run: _setup/setup.sh_ this will install all the binaries, dependencies and files for python and R and Scala that you will need. This will take a bit of time. Next it is recommended to activate the virtual environment _. venv/bin/activate_ Next to get all the data from wikipedia to stack exchange all in one download run _data/download_all.sh_ this is a huge download. The vast majority of its size comes from the wikipedia edit history, which is around 70GB. For this part to work properly you may need to find the most current _enwiki-stub-meta-history.xml.gz_ file yourself on the wikimedia site. Use the file _get_wikipedia_data.sh_ from the _src/chapter1_ directory. Open it and where it is coded _'curl https://dumps.wikimedia.org/enwiki/$backup_date/enwiki-$backup_date-stub-meta-history.xml.gz \
    -o data/wikipedia/enwiki-stub-meta-history.xml.gz'_ change the dump URL to the most recent one you've found on wikimedia. Make sure it is a _xml.gz_   filetype. Once this section has been edited then _the_download_all.sh_  script in the _setup_ directory will run.This should now give you everything you need to run the code from the book in the various chapter sections. As a final recommendation, once running python if errors come up immediately after running the code from the command line one tip is to run it with python2 rather than python3. 
    
